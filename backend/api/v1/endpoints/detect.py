"""Text content detection endpoints."""

import logging
from typing import Dict, Optional

from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from db.session import get_db
from services.text_detection_service import TextDetectionService


class DetectRequest(BaseModel):
    """Request for text content detection."""

    post_id: str = Field(..., description="Facebook post ID")
    content: str = Field(..., description="Text content to analyze")
    author: Optional[str] = Field(None, description="Post author")
    metadata: Optional[dict] = Field(None, description="Additional metadata")


class DetectResponse(BaseModel):
    """Response for text content detection."""

    post_id: str = Field(..., description="Facebook post ID")
    verdict: str = Field(..., description="Detection verdict: ai_slop, human_content, or uncertain")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score")
    explanation: str = Field(..., description="Explanation for the verdict")
    timestamp: str = Field(..., description="Analysis timestamp")
    debug_info: Optional[dict] = Field(None, description="Debug information")


logger = logging.getLogger(__name__)

router = APIRouter(tags=["text-detection"])

# Initialize service
detection_service = TextDetectionService()


@router.post("/analyze", response_model=DetectResponse)
async def analyze_text(
    request: DetectRequest,
    db: AsyncSession = Depends(get_db),
) -> DetectResponse:
    """
    Analyze text content for AI generation patterns.

    This endpoint analyzes the provided text content and determines whether it was
    likely generated by AI, written by a human, or uncertain.
    """
    try:
        logger.info(f"Analyzing post {request.post_id} with {len(request.content)} characters")

        result = await detection_service.detect(request, db)

        logger.info(f"Analysis complete for post {request.post_id}: verdict={result.verdict}, confidence={result.confidence:.2%}")

        return result

    except Exception as e:
        logger.error(f"Error analyzing post {request.post_id}: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to analyze content: {str(e)}")


@router.get("/cache/stats")
async def get_cache_stats(
    db: AsyncSession = Depends(get_db),
) -> Dict:
    """
    Get cache statistics.

    Returns statistics about the detection cache including hit rates,
    total cached entries, and distribution of verdicts.
    """
    try:
        stats = await detection_service.get_cache_stats(db)

        return {
            "success": True,
            "data": stats,
        }

    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to get cache statistics: {str(e)}")
