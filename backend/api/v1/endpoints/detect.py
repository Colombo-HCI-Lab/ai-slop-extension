"""Content detection endpoints."""

import logging
from typing import Dict

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from db.session import get_db
from schemas.content_detection import ContentDetectionRequest, ContentDetectionResponse
from services.content_detection_service import ContentDetectionService


logger = logging.getLogger(__name__)

router = APIRouter(tags=["content-detection"])

# Initialize service
detection_service = ContentDetectionService()


@router.post("/analyze", response_model=ContentDetectionResponse)
async def analyze_content(
    request: ContentDetectionRequest,
    db: AsyncSession = Depends(get_db),
) -> ContentDetectionResponse:
    """
    Analyze content for AI generation patterns.

    This endpoint analyzes text content, images, and videos to determine whether
    the content was likely generated by AI, created by humans, or uncertain.
    Supports simultaneous analysis of:
    - Text content (always analyzed)
    - Image URLs (analyzed if provided)
    - Video URLs (analyzed if provided)
    """
    try:
        # Debug logging to see what's received
        logger.info(f"Received request data: {request.dict()}")
        logger.info(
            f"Analyzing post {request.post_id} with {len(request.content)} characters, {len(request.image_urls or [])} images, {len(request.video_urls or [])} videos"
        )

        result = await detection_service.detect(request, db)

        logger.info(f"Analysis complete for post {request.post_id}: verdict={result.verdict}, confidence={result.confidence:.2%}")

        return result

    except Exception as e:
        logger.error(f"Error analyzing post {request.post_id}: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to analyze content: {str(e)}")


@router.get("/cache/stats")
async def get_cache_stats(
    db: AsyncSession = Depends(get_db),
) -> Dict:
    """
    Get cache statistics.

    Returns statistics about the detection cache including hit rates,
    total cached entries, and distribution of verdicts.
    """
    try:
        # Use the text service directly for cache stats since content service delegates to it
        from services.text_detection_service import TextDetectionService

        text_service = TextDetectionService()
        stats = await text_service.get_cache_stats(db)

        return {
            "success": True,
            "data": stats,
        }

    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to get cache statistics: {str(e)}")
